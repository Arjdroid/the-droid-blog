+++
draft = "true"
author = "Arjdroid"
title = "Running a Local Large Language Model (AKA Local AI ChatBot) PART TWO"
date = "2024-02-25"
description = "This is blog post details my journey in setting up and running an (LLM) large-language-model on my very own computer in order to have my own alternative to ChatGPT"
tags = [
    "ai",
    "llm",
    "gpt",
]
categories = [
    "ai",
    "technical",]
series = ["AI"]
aliases = ["Local-LLM"]
+++

## Introduction

I have 48 GB of RAM now and I can run the fabled Mixtral 8x7B model. It is quite fabulous. Although, because it is now running on my CPU instead of my GPU, quite slow (on par with the GPT-4 non-turbo model on chat.openai.com). I have yet to run actual, 'objective' performance benchmarks so what follows consists only of my personal opinions and may be taken with a grain of salt.
