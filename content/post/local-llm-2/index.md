+++
draft = "true"
author = "Arjdroid"
title = "Running a Local Large Language Model (AKA Local AI ChatBot) PART TWO"
date = "2024-06-11"
description = "This is blog post details my journey in setting up and running an (LLM) large-language-model on my very own computer in order to have my own alternative to ChatGPT"
tags = [
    "ai",
    "llm",
    "gpt",
]
categories = [
    "ai",
    "technical",]
series = ["AI"]
aliases = ["Local-LLM"]
image = "llama2-on-mac.png"
+++

## Introduction

A lot of changes have taken place in the fast-paced and exciting world of locally machine learning since my last post.

I myself have acquired 48 gigs of DRAM and am able to run many more interesting local LLMs on my workstation, including Mixtral 8x7B. There have also been the major releases of new LLMs including llama3, Mistral 7B 0.2

// it appears that each subsequent attempt of mine to write on this topic becomes woefully out of date rather quickly.
